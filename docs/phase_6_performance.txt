Phase 6 — Profiling & Optimization (In Progress)

Goal
Identify performance bottlenecks in the N-body simulation code before attempting
any optimization.
The purpose of this phase is to base optimization decisions on measured data
rather than assumptions.

Approach
- Use Python's built-in cProfile to measure runtime at the function level.
- Profile the full simulation pipeline (solver + integrator + diagnostics).
- Sort results by cumulative runtime to identify dominant cost centers.
- Perform profiling separately for Direct and Barnes–Hut solvers.

Initial Profiling Setup (DirectSolver)
Integrator: Leapfrog (Velocity Verlet)
Solver: Direct O(N^2)
N = 300
Steps = 300
dt = 2e-3
Softening = 1e-3

Summary of Results (DirectSolver)
- Total runtime ~95 seconds for the profiled run.
- The pairwise force calculation (compute_accelerations) dominates runtime.
- This confirms the expected O(N^2) scaling cost of the DirectSolver.
- Leapfrog integration contributes secondary cost due to repeated force
  evaluations and synchronization.
- Diagnostic computations (energy, momentum, COM) contribute noticeable but
  smaller overhead.
- State copying and object creation appear in the profile but are not the
  primary bottleneck.

Interpretation
The profiling results match theoretical expectations:
the majority of time is spent computing pairwise gravitational forces.
This validates the correctness of the performance model and provides a clear
target for optimization efforts in Phase 6.

Planned Next Steps
- Profile the Barnes–Hut solver to identify tree construction and traversal
  costs.
- Use profiling data to prioritize low-risk optimizations.
- Avoid algorithmic changes; focus on reducing Python overhead.
- Keep diagnostics intact to preserve validation correctness.

Notes
This phase focuses on understanding performance behavior.
Optimization work will be incremental and guided by profiling results.






Initial Profiling Setup (Barnes–Hut)
Integrator: Leapfrog (Velocity Verlet)
Solver: Barnes–Hut Octree (θ = 0.7)
N = 1000
Steps = 300
dt = 2e-3
Softening = 1e-3

Summary of Results (Barnes–Hut)
- Total runtime ~1038 seconds for the profiled run.
- Runtime is dominated by recursive tree traversal during force evaluation.
- OctreeNode.compute_accelerations accounts for the majority of runtime.
- Tree construction (insert, subdivide, mass/COM updates) contributes noticeable
  but smaller overhead.
- Barnes–Hut results in extremely high function call counts due to recursion and
  repeated node visits.
- Leapfrog integration amplifies traversal cost due to repeated acceleration
  evaluations per timestep.
- Diagnostic computations remain expensive but are not the dominant cost.

Comparison to DirectSolver
- DirectSolver runtime is dominated by pairwise force computation.
- Barnes–Hut runtime is dominated by Python recursion and object overhead.
- Barnes–Hut shows better algorithmic scaling, but Python overhead significantly
  reduces its practical performance advantage at this stage.

Interpretation
The profiling results explain why the current Barnes–Hut implementation is
slower than expected despite improved asymptotic scaling.
The algorithmic advantage is outweighed by Python-level overhead from recursion,
object creation, and attribute access.

Planned Next Steps
- Identify low-risk ways to reduce tree traversal overhead.
- Consider optimization strategies that reduce Python call counts.
- Use profiling data to choose which solver to optimize first.


Profiling Results (Diagnostics Disabled)

Direct Solver
- Total runtime ~72 seconds for the profiling run.
- Hotspot is physics.compute_accelerations.
- Confirms that DirectSolver performance is dominated by the O(N^2) force loop.
- Integrator overhead is present but secondary.

Barnes–Hut Solver
- Total runtime ~760 seconds under profiling.
- Extremely large function call count due to recursive tree traversal.
- octree.compute_accelerations dominates cumulative runtime.
- Tree construction costs are visible but not the primary bottleneck.
- Profiling overhead disproportionately affects Barnes–Hut due to recursion.

Interpretation
With diagnostics removed, profiling confirms that:
- DirectSolver cost comes from pairwise force computation.
- Barnes–Hut cost comes from Python recursion and object-level traversal.
- Barnes–Hut algorithmic advantages are masked by Python-level overhead during
  profiling.

Implication for Phase 6
Optimization should focus on:
- Reducing Python overhead in Barnes–Hut traversal.
- Reducing attribute access and repeated computations.
- Keeping the algorithm unchanged while improving execution efficiency.




Optimization Attempt 1 — Barnes–Hut Tree Traversal (compute_accelerations)

Change:
Made small local optimizations inside octree.compute_accelerations:
- Cached frequently accessed attributes into local variables.
- Removed duplicate distance calculations.
- Precomputed softening^2.
- Returned early in the approximation case to reduce work.

Reason:
Profiling showed that octree.compute_accelerations dominates Barnes–Hut runtime.
The goal was to reduce Python overhead per call without changing the algorithm.

Expected Effect:
Slight reduction in runtime due to fewer attribute lookups and repeated math
operations in a function that is called many times.

Notes:
This change does not modify the Barnes–Hut opening-angle rule or force
calculation.
Behavior and accuracy should remain unchanged.

Status:
Pending re-profiling.

Verification:
Ran Barnes–Hut with N = 10 and diagnostics enabled after modifying
compute_accelerations.
Simulation completed without errors and diagnostics behaved as expected.
No obvious change in behavior observed.

Results:
Re-profiled Barnes–Hut with the N = 1000 configuration and diagnostics after optimizing
compute_accelerations.

- Time spent inside octree.compute_accelerations decreased.
- Overall profiling runtime increased.
- Function call count increased significantly.

Interpretation:
The optimization reduced work per call but did not reduce the number of
recursive calls.
Under cProfile, the increased call count amplified profiler overhead, masking
any real runtime improvement.

Conclusion:
This optimization alone is insufficient under profiling conditions.
Further improvements must focus on reducing recursion and function call count,
not just per-call arithmetic cost.


Results (Real Runtime, No Profiling):

Barnes–Hut timing comparison using identical configuration:

- Original implementation: ~349.5 s
- Optimized implementation: ~233.1 s

This corresponds to an improvement of roughly 33%.

Interpretation:
Although profiling results were inconclusive due to profiler overhead,
real runtime measurements show that reducing per-call overhead in
octree.compute_accelerations leads to a substantial speedup.

Conclusion:
The optimization is effective and should be kept.
Further Phase 6 work should focus on reducing recursion and function
call count to achieve additional gains.